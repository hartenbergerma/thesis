{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import Accuracy \n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.train import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train.lightning import (\n",
    "    RayDDPStrategy,\n",
    "    RayLightningEnvironment,\n",
    "    RayTrainReportCallback,\n",
    "    prepare_trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(pl.LightningModule):\n",
    "    def __init__(self, input_dim, output_dim, loss_weight, config):\n",
    "        super().__init__()\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.loss_weight = Tensor(loss_weight).to(device)\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=output_dim, top_k=1)\n",
    "        self.f1_score = MulticlassF1Score(num_classes=output_dim, average=\"macro\")\n",
    "\n",
    "        self.weight_decay = config[\"weight_decay\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "        # self.num_layers = config[\"num_layers\"]\n",
    "        self.hidden_dim = config[\"hidden_dim\"]\n",
    "        self.last_layer_dim = config[\"last_layer_dim\"]\n",
    "\n",
    "        input_layer = nn.Linear(input_dim, self.hidden_dim)\n",
    "        # hidden_layers = []\n",
    "        # for i in range(self.num_layers):\n",
    "        #     hidden_layers.append(nn.Linear(self.hidden_dim, self.hidden_dim))\n",
    "        #     hidden_layers.append(nn.ReLU())\n",
    "        last_layer = nn.Linear(self.hidden_dim, self.last_layer_dim)\n",
    "        output_layer = nn.Linear(self.last_layer_dim, output_dim)\n",
    "        # self.layers = nn.Sequential(input_layer, nn.ReLU(), *hidden_layers, last_layer, nn.ReLU(), output_layer)\n",
    "        self.layers = nn.Sequential(input_layer, nn.BatchNorm1d(self.hidden_dim), nn.ReLU(), last_layer, nn.BatchNorm1d(self.last_layer_dim), nn.ReLU(), output_layer)\n",
    "\n",
    "        self.eval_loss = []\n",
    "        self.eval_accuracy = []\n",
    "\n",
    "        self.epoch_logits = []\n",
    "        self.epoch_labels = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y, weight=self.loss_weight)\n",
    "        accuracy = self.accuracy(logits, y)\n",
    "        self.log(\"train/train_loss\", loss)\n",
    "        self.log(\"train/train_accuracy\", accuracy)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y, weight=self.loss_weight)\n",
    "        accuracy = self.accuracy(logits, y)\n",
    "        self.eval_loss.append(loss)\n",
    "        self.eval_accuracy.append(accuracy)\n",
    "\n",
    "        self.epoch_logits.append(logits)\n",
    "        self.epoch_labels.append(y)\n",
    "\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y, weight=self.loss_weight)\n",
    "        accuracy = self.accuracy(logits, y)\n",
    "        self.log(\"test/test_loss\", loss)\n",
    "        self.log(\"test/test_accuracy\", accuracy)\n",
    "        self.eval_loss.append(loss)\n",
    "        self.eval_accuracy.append(accuracy)\n",
    "\n",
    "        self.epoch_logits.append(logits)\n",
    "        self.epoch_labels.append(y)\n",
    "\n",
    "        return {\"test_loss\": loss, \"test_accuracy\": accuracy}\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        display(len(self.eval_loss), torch.stack(self.eval_loss).shape)\n",
    "        avg_loss = torch.stack(self.eval_loss).mean()\n",
    "        avg_acc = torch.stack(self.eval_accuracy).mean()\n",
    "        self.log(\"val/val_loss\", avg_loss, sync_dist=True)\n",
    "        self.log(\"val/val_accuracy\", avg_acc, sync_dist=True)\n",
    "        self.eval_loss.clear()\n",
    "        self.eval_accuracy.clear()\n",
    "\n",
    "        epoch_logits = torch.cat(self.epoch_logits, dim=0)\n",
    "        epoch_labels = torch.cat(self.epoch_labels, dim=0)\n",
    "        f1_score = self.f1_score(epoch_logits, epoch_labels)\n",
    "        self.log(\"val/f1_score\", f1_score, sync_dist=True)\n",
    "        self.epoch_logits.clear()\n",
    "        self.epoch_labels.clear()\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        avg_loss = torch.stack(self.eval_loss).mean()\n",
    "        avg_acc = torch.stack(self.eval_accuracy).mean()\n",
    "        self.log(\"test/test_loss\", avg_loss, sync_dist=True)\n",
    "        self.log(\"test/test_accuracy\", avg_acc, sync_dist=True)\n",
    "        self.eval_loss.clear()\n",
    "        self.eval_accuracy.clear()\n",
    "\n",
    "        epoch_logits = torch.cat(self.epoch_logits, dim=0)\n",
    "        epoch_labels = torch.cat(self.epoch_labels, dim=0)\n",
    "        f1_score = self.f1_score(epoch_logits, epoch_labels)\n",
    "        self.log(\"test/f1_score\", f1_score, sync_dist=True)\n",
    "        self.epoch_logits.clear()\n",
    "        self.epoch_labels.clear()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        i = 0\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                i += 1\n",
    "                self.logger.experiment.add_histogram(f\"layer_{i}/weight\", layer.weight, self.current_epoch)\n",
    "                self.logger.experiment.add_histogram(f\"layer_{i}/bias\", layer.bias, self.current_epoch)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), self.lr, weight_decay=self.weight_decay)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class helicoid_Dataset(Dataset):\n",
    "    def __init__(self, patient_folders, files, transform=None, mode='labeled'):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        for patient_folder in patient_folders:\n",
    "            print(f\"loading image {patient_folder}\")\n",
    "            patient_folder = os.path.join('/home/martin_ivan/code/own_labels/npj_database/', patient_folder)\n",
    "            img_data = []\n",
    "            img_labels = np.load(os.path.join(patient_folder, 'gtMap.npy')).astype(int)\n",
    "            for file in files:\n",
    "                img_data_all = np.load(os.path.join(patient_folder, file))\n",
    "                if mode == 'labeled':\n",
    "                    # img_data.append(img_data_all[(img_labels !=0) & (img_labels != 4)])\n",
    "                    img_data.append(img_data_all[(img_labels !=0)])\n",
    "                elif mode == 'all':\n",
    "                    img_data.append(img_data_all.reshape(-1, img_data_all.shape[-1]))\n",
    "                else:\n",
    "                    raise ValueError(\"Unknown mode\")\n",
    "            img_data = np.concatenate(img_data, axis=1)\n",
    "\n",
    "            self.data.append(img_data)\n",
    "            if mode == 'labeled':\n",
    "                # self.labels.append(img_labels[(img_labels !=0) & (img_labels != 4)])\n",
    "                self.labels.append(img_labels[(img_labels !=0)])\n",
    "            elif mode == 'all':\n",
    "                self.labels.append(img_labels.reshape(-1))\n",
    "            else:\n",
    "                raise ValueError(\"Unknown mode\")\n",
    "                    \n",
    "        self.data = np.concatenate(self.data, axis=0)\n",
    "        self.labels = np.concatenate(self.labels, axis=0)\n",
    "        print(f\"------------- label counts: {np.unique(self.labels, return_counts=True)} -------------\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx] - 1\n",
    "        if self.transform:\n",
    "            x, y = self.transform((x, y))\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        x, y = sample\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "class HelicoidDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, files, batch_size=64, fold=\"fold1\"):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.fold = fold\n",
    "        self.files = files\n",
    "        self.transform = ToTensor()\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        with open('/home/martin_ivan/code/own_labels/folds.json') as f:\n",
    "            folds = json.load(f)\n",
    "\n",
    "        if stage==\"fit\":\n",
    "            self.dataset_train = helicoid_Dataset(folds[self.fold][\"train\"], self.files, transform=self.transform)\n",
    "            self.dataset_val = helicoid_Dataset(folds[self.fold][\"val\"], self.files, transform=self.transform)\n",
    "        if stage==\"test\":\n",
    "            self.dataset_test = helicoid_Dataset(folds[self.fold][\"test\"], self.files, transform=self.transform)\n",
    "        if stage==\"predict\":\n",
    "            self.dataset_predict = helicoid_Dataset(folds[self.fold][\"test\"], self.files, transform=self.transform, mode='all')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset_train, batch_size=self.batch_size, shuffle=True, num_workers=16)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.dataset_val, batch_size=self.batch_size, shuffle=False, num_workers=16)\n",
    "    \n",
    "    def test_dataloader(self): \n",
    "        return DataLoader(self.dataset_test, batch_size=self.batch_size, shuffle=False, num_workers=16)\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.dataset_predict, batch_size=self.batch_size, shuffle=False, num_workers=16)\n",
    "    \n",
    "    def sample_size(self):\n",
    "        return self.dataset_train.data.shape[1]\n",
    "    \n",
    "    def class_distribution(self):\n",
    "        return np.unique(self.dataset_train.labels, return_counts=True)[1]\n",
    "    \n",
    "    def num_classes(self):\n",
    "        return len(np.unique(self.dataset_train.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading image 005-01\n",
      "loading image 007-01\n",
      "loading image 012-01\n",
      "loading image 012-02\n",
      "loading image 018-01\n",
      "loading image 018-02\n",
      "loading image 019-01\n",
      "loading image 020-01\n",
      "loading image 013-01\n",
      "loading image 015-01\n",
      "loading image 017-01\n",
      "loading image 034-02\n",
      "loading image 035-01\n",
      "loading image 036-01\n",
      "loading image 036-02\n",
      "loading image 038-01\n",
      "loading image 040-01\n",
      "loading image 040-02\n",
      "loading image 043-01\n",
      "loading image 043-02\n",
      "loading image 043-04\n",
      "loading image 050-01\n",
      "loading image 051-01\n",
      "loading image 053-01\n",
      "loading image 055-01\n",
      "loading image 056-01\n",
      "loading image 056-02\n",
      "loading image 057-01\n",
      "loading image 058-02\n",
      "------------- label counts: (array([1, 2, 3, 4]), array([179536,  29262,  70340, 277811])) -------------\n",
      "loading image 004-02\n",
      "loading image 008-01\n",
      "loading image 008-02\n",
      "loading image 022-01\n",
      "loading image 022-02\n",
      "loading image 039-01\n",
      "loading image 039-02\n",
      "loading image 041-01\n",
      "loading image 041-02\n",
      "------------- label counts: (array([1, 2, 3, 4]), array([39410,  3459,  9548, 43357])) -------------\n"
     ]
    }
   ],
   "source": [
    "files = [\"preprocessed.npy\"]#, \"heatmaps_osp.npy\", \"heatmaps_osp_diff.npy\", \"heatmaps_osp_diff_mc.npy\", \"heatmaps_icem.npy\", \"heatmaps_icem_diff.npy\", \"heatmaps_icem_diff_mc.npy\"]\n",
    "dm = HelicoidDataModule(files=files, fold=\"fold1\")\n",
    "dm.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup training function for ray tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "    model = ClassificationModel(input_dim=dm.sample_size(), output_dim=dm.num_classes(), loss_weight=dm.class_distribution(), config=config)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        devices=\"auto\",\n",
    "        accelerator=\"auto\",\n",
    "        strategy=RayDDPStrategy(),\n",
    "        callbacks=[RayTrainReportCallback()],\n",
    "        plugins=[RayLightningEnvironment()],\n",
    "        enable_progress_bar=False,\n",
    "        \n",
    "    )\n",
    "    trainer = prepare_trainer(trainer)\n",
    "    trainer.fit(model, dm.train_dataloader(), dm.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-03-30 20:54:23</td></tr>\n",
       "<tr><td>Running for: </td><td>00:27:18.53        </td></tr>\n",
       "<tr><td>Memory:      </td><td>360.9/503.7 GiB    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 20.000: None | Iter 5.000: -2.769998550415039<br>Logical resource usage: 17.0/128 CPUs, 1.0/8 GPUs (0.0/1.0 accelerator_type:RTX)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">  train_loop_config/hi\n",
       "dden_dim</th><th style=\"text-align: right;\">  train_loop_config/la\n",
       "st_layer_dim</th><th style=\"text-align: right;\">  train_loop_config/lr</th><th style=\"text-align: right;\">            train_loop_config/we\n",
       "ight_decay</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train/train_loss</th><th style=\"text-align: right;\">  train/train_accuracy</th><th style=\"text-align: right;\">  val/val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_7dc19_00000</td><td>RUNNING </td><td>131.159.10.130:647956</td><td style=\"text-align: right;\">9</td><td style=\"text-align: right;\">9</td><td style=\"text-align: right;\">            0.00130116</td><td style=\"text-align: right;\">0.000934396</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         1512.67</td><td style=\"text-align: right;\">          0.289957</td><td style=\"text-align: right;\">              0.857143</td><td style=\"text-align: right;\">       5.97782</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[36m(TorchTrainer pid=647956)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=647956)\u001b[0m - (ip=131.159.10.130, pid=648046) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m [W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m /home/martin_ivan/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m   warnings.warn(_create_warning_msg(\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m /home/martin_ivan/anaconda3/envs/thesis/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m Missing logger folder: /tmp/ray/session_2024-03-30_20-03-52_482813_614347/artifacts/2024-03-30_20-27-05/TorchTrainer_2024-03-30_20-27-05/working_dirs/TorchTrainer_7dc19_00000_0_hidden_dim=9,last_layer_dim=9,lr=0.0013,weight_decay=0.0009_2024-03-30_20-27-05/lightning_logs\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m [rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m   | Name     | Type               | Params\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m ------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m 0 | accuracy | MulticlassAccuracy | 0     \n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m 1 | f1_score | MulticlassF1Score  | 0     \n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m 2 | layers   | Sequential         | 3.6 K \n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m ------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m 3.6 K     Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m 3.6 K     Total params\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m 0.014     Total estimated model params size (MB)\n",
      "2024-03-30 20:29:36,512\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/martin_ivan/ray_results/TorchTrainer_2024-03-30_20-27-05/TorchTrainer_7dc19_00000_0_hidden_dim=9,last_layer_dim=9,lr=0.0013,weight_decay=0.0009_2024-03-30_20-27-05/checkpoint_000000)\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/martin_ivan/ray_results/TorchTrainer_2024-03-30_20-27-05/TorchTrainer_7dc19_00000_0_hidden_dim=9,last_layer_dim=9,lr=0.0013,weight_decay=0.0009_2024-03-30_20-27-05/checkpoint_000001)\n",
      "2024-03-30 20:34:07,303\tWARNING experiment_state.py:205 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this error by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0).\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/martin_ivan/ray_results/TorchTrainer_2024-03-30_20-27-05/TorchTrainer_7dc19_00000_0_hidden_dim=9,last_layer_dim=9,lr=0.0013,weight_decay=0.0009_2024-03-30_20-27-05/checkpoint_000002)\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/martin_ivan/ray_results/TorchTrainer_2024-03-30_20-27-05/TorchTrainer_7dc19_00000_0_hidden_dim=9,last_layer_dim=9,lr=0.0013,weight_decay=0.0009_2024-03-30_20-27-05/checkpoint_000003)\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/martin_ivan/ray_results/TorchTrainer_2024-03-30_20-27-05/TorchTrainer_7dc19_00000_0_hidden_dim=9,last_layer_dim=9,lr=0.0013,weight_decay=0.0009_2024-03-30_20-27-05/checkpoint_000004)\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/martin_ivan/ray_results/TorchTrainer_2024-03-30_20-27-05/TorchTrainer_7dc19_00000_0_hidden_dim=9,last_layer_dim=9,lr=0.0013,weight_decay=0.0009_2024-03-30_20-27-05/checkpoint_000005)\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/martin_ivan/ray_results/TorchTrainer_2024-03-30_20-27-05/TorchTrainer_7dc19_00000_0_hidden_dim=9,last_layer_dim=9,lr=0.0013,weight_decay=0.0009_2024-03-30_20-27-05/checkpoint_000006)\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/martin_ivan/ray_results/TorchTrainer_2024-03-30_20-27-05/TorchTrainer_7dc19_00000_0_hidden_dim=9,last_layer_dim=9,lr=0.0013,weight_decay=0.0009_2024-03-30_20-27-05/checkpoint_000007)\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/martin_ivan/ray_results/TorchTrainer_2024-03-30_20-27-05/TorchTrainer_7dc19_00000_0_hidden_dim=9,last_layer_dim=9,lr=0.0013,weight_decay=0.0009_2024-03-30_20-27-05/checkpoint_000008)\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/martin_ivan/ray_results/TorchTrainer_2024-03-30_20-27-05/TorchTrainer_7dc19_00000_0_hidden_dim=9,last_layer_dim=9,lr=0.0013,weight_decay=0.0009_2024-03-30_20-27-05/checkpoint_000009)\n",
      "\u001b[36m(RayTrainWorker pid=648046)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/martin_ivan/ray_results/TorchTrainer_2024-03-30_20-27-05/TorchTrainer_7dc19_00000_0_hidden_dim=9,last_layer_dim=9,lr=0.0013,weight_decay=0.0009_2024-03-30_20-27-05/checkpoint_000010)\n",
      "2024-03-30 20:54:23,603\tWARNING tune.py:229 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2024-03-30 20:54:23,609\tINFO tune.py:1016 -- Wrote the latest version of all result files and experiment state to '/home/martin_ivan/ray_results/TorchTrainer_2024-03-30_20-27-05' in 0.0044s.\n",
      "2024-03-30 20:54:33,622\tINFO tune.py:1048 -- Total run time: 1648.56 seconds (1638.53 seconds for the tuning loop).\n",
      "2024-03-30 20:54:33,624\tWARNING tune.py:1063 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"/home/martin_ivan/ray_results/TorchTrainer_2024-03-30_20-27-05\", trainable=...)\n"
     ]
    }
   ],
   "source": [
    "def tune_hyperparameters():\n",
    "\n",
    "    search_space = {\n",
    "    \"hidden_dim\": tune.choice([9]),\n",
    "    # \"num_layers\": tune.choice([0,2,4,8,16]),\n",
    "    \"last_layer_dim\": tune.choice([9]),\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "    \"weight_decay\": tune.loguniform(1e-5, 1e-2),\n",
    "    }   \n",
    "\n",
    "    ray_num_workers = 1\n",
    "    num_epochs = 50\n",
    "    num_samples = 20\n",
    "\n",
    "    scheduler = ASHAScheduler(max_t=num_epochs, grace_period=5, reduction_factor=4)\n",
    "\n",
    "    run_config = RunConfig(\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"val/val_loss\",\n",
    "            checkpoint_score_order=\"min\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ray_trainer = TorchTrainer(\n",
    "        train_func,\n",
    "        scaling_config=ScalingConfig(num_workers=1, use_gpu=True, resources_per_worker={\"CPU\": 16, \"GPU\": 1}),\n",
    "        run_config=run_config,)\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        ray_trainer,\n",
    "        param_space={\"train_loop_config\": search_space},\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"val/val_loss\",\n",
    "            mode=\"min\",\n",
    "            num_samples=num_samples,\n",
    "            scheduler=scheduler,\n",
    "            max_concurrent_trials=ray_num_workers\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return tuner.fit()\n",
    "\n",
    "result = tune_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name     | Type               | Params\n",
      "------------------------------------------------\n",
      "0 | accuracy | MulticlassAccuracy | 0     \n",
      "1 | f1_score | MulticlassF1Score  | 0     \n",
      "2 | layers   | Sequential         | 3.6 K \n",
      "------------------------------------------------\n",
      "3.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.6 K     Total params\n",
      "0.014     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d257205e088849f286459419fd0448ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7daf5aa840b146d98e2fd530bf0e7d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00b4dcec6b24a6a8a5b99bc08fe3c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7c9fc05e0f434fb5c7c4582f083106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f980efbf010>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/martin_ivan/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/martin_ivan/anaconda3/envs/thesis/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/martin_ivan/anaconda3/envs/thesis/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/martin_ivan/anaconda3/envs/thesis/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/home/martin_ivan/anaconda3/envs/thesis/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/martin_ivan/anaconda3/envs/thesis/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb633bb9d1b14013a50f7cd06e422454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3daae075e6fa4bb2946fc51dad3dbc8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b19379ba4474a20a9e6d4f1839e0218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5204c7574e50480f89f0e2f07f888d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab3de4956514fe285c2ec8605746887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bb9aa53b2140e6b17bce641e4f12ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274bd19001a24635823cd06b2aeda581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7e418b492e41eabe0b0c73b23ea313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019fbc9be2f1475c85013feb400ef33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e441a4680c843d689b5d56a0c1e89d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e986dc095a24420c949850ffe2333c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f03923345d4d73b1356ab64a2d80c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e380bae807ea42278aeae380f2568bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a436c7edce4b12b715be1357b21ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8efff8509941688c2a8e369cdbf26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edaf88e577ba4662a2c544523cb1f7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce738775d1824cb29a94c7982e56e933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd53f458423949608b733e85ae7969b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import model\n",
    "from model import ClassificationModel\n",
    "reload(model)\n",
    "\n",
    "def train(config):\n",
    "    model = ClassificationModel(input_dim=dm.sample_size(), output_dim=dm.num_classes(), loss_weight=dm.class_distribution(), config=config)\n",
    "    logger = TensorBoardLogger(config[\"log_dir\"], name=\"my_model\")\n",
    "    trainer = pl.Trainer(logger=logger, max_epochs=config[\"num_epochs\"], devices=1)\n",
    "    trainer.fit(model, dm.train_dataloader(), dm.val_dataloader())\n",
    "    trainer.save_checkpoint(\"./classification/model.ckpt\")\n",
    "    return model\n",
    "\n",
    "config = {\n",
    "    \"hidden_dim\": 9,\n",
    "    \"num_layers\": 0,\n",
    "    \"last_layer_dim\": 9,\n",
    "    \"lr\": 1e-5,\n",
    "    \"weight_decay\": 0,\n",
    "    \"num_epochs\": 20,\n",
    "    \"log_dir\": \"./classification/tb_logs/\"\n",
    "}\n",
    "\n",
    "model = train(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
